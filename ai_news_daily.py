#!/usr/bin/env python3
"""
AI Daily News Summary Generator with Deduplication
Uses Brave Search API to fetch latest AI news and generates a comprehensive summary
"""

import requests
import json
import os
import sys
import hashlib
from datetime import datetime, timedelta
from openai import OpenAI

# Import Feishu push module
try:
    from feishu_push import send_news_summary
    FEISHU_ENABLED = True
except ImportError:
    FEISHU_ENABLED = False
    print("è­¦å‘Š: é£ä¹¦æ¨é€æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡é£ä¹¦æ¨é€åŠŸèƒ½")

# Configuration
BRAVE_API_KEY = os.getenv("BRAVE_API_KEY")
if not BRAVE_API_KEY:
    print("âŒ é”™è¯¯: BRAVE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    sys.exit(1)

BRAVE_BASE_URL = "https://api.search.brave.com/res/v1/web/search"
LOCK_FILE = os.getenv("LOCK_FILE", ".ai_news_lock")
HISTORY_FILE = os.getenv("HISTORY_FILE", ".ai_news_history.json")

# Initialize OpenAI client
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL")
if api_key and base_url:
    client = OpenAI(api_key=api_key, base_url=base_url)
else:
    client = OpenAI()


def check_daily_execution():
    """Check if the script has already been executed today"""
    if os.path.exists(LOCK_FILE):
        try:
            with open(LOCK_FILE, 'r') as f:
                last_run = f.read().strip()
            last_run_date = datetime.strptime(last_run, "%Y-%m-%d").date()
            today = datetime.now().date()
            if last_run_date == today:
                print(f"âš ï¸ è„šæœ¬ä»Šå¤©å·²ç»è¿è¡Œè¿‡äº†ï¼ˆ{last_run}ï¼‰")
                print(f"âš ï¸ ä¸ºäº†ä¿æŠ¤ API é…é¢ï¼Œæ¯å¤©åªèƒ½è¿è¡Œä¸€æ¬¡")
                return True
        except Exception as e:
            print(f"è­¦å‘Š: è¯»å–é”æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    return False


def update_lock_file():
    """Update the lock file with today's date"""
    try:
        with open(LOCK_FILE, 'w') as f:
            f.write(datetime.now().strftime("%Y-%m-%d"))
    except Exception as e:
        print(f"è­¦å‘Š: æ›´æ–°é”æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def load_news_history():
    """Load news history from JSON file"""
    if not os.path.exists(HISTORY_FILE):
        print("ğŸ“ å†å²è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        return {"news_history": [], "last_cleanup": datetime.now().strftime("%Y-%m-%d")}
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
        print(f"âœ… åŠ è½½äº† {len(history.get('news_history', []))} æ¡å†å²æ–°é—»è®°å½•")
        return history
    except Exception as e:
        print(f"âš ï¸ è¯»å–å†å²è®°å½•æ—¶å‡ºé”™: {e}")
        return {"news_history": [], "last_cleanup": datetime.now().strftime("%Y-%m-%d")}


def save_news_history(history):
    """Save news history to JSON file"""
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"âœ… å†å²è®°å½•å·²æ›´æ–°ï¼ˆå…± {len(history.get('news_history', []))} æ¡ï¼‰")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å†å²è®°å½•æ—¶å‡ºé”™: {e}")


def cleanup_old_history(history, days=7):
    """Remove news older than specified days from history"""
    cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    original_count = len(history.get("news_history", []))
    history["news_history"] = [
        news for news in history.get("news_history", [])
        if news.get("last_seen", "2000-01-01") >= cutoff_date
    ]
    removed_count = original_count - len(history["news_history"])
    if removed_count > 0:
        print(f"ğŸ§¹ æ¸…ç†äº† {removed_count} æ¡è¶…è¿‡ {days} å¤©çš„æ—§è®°å½•")
    history["last_cleanup"] = datetime.now().strftime("%Y-%m-%d")
    return history


def calculate_title_similarity(title1, title2):
    """Calculate similarity between two titles using Jaccard similarity"""
    words1 = set(title1.lower().split())
    words2 = set(title2.lower().split())
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    if not union:
        return 0.0
    return len(intersection) / len(union)


def get_title_hash(title):
    """Generate a hash for the title"""
    return hashlib.md5(title.lower().encode('utf-8')).hexdigest()


def is_duplicate_news(article, history, similarity_threshold=0.85):
    """Check if an article is a duplicate"""
    url = article.get('url', '')
    title = article.get('title', '')
    if not url or not title:
        return False
    
    for news in history.get("news_history", []):
        if news.get("url") == url:
            return True
        similarity = calculate_title_similarity(title, news.get("title", ""))
        if similarity >= similarity_threshold:
            return True
    return False


def filter_duplicate_news(articles, history):
    """Filter out duplicate news"""
    print("ğŸ” æ­£åœ¨æ£€æŸ¥é‡å¤æ–°é—»...")
    filtered = []
    duplicate_count = 0
    for article in articles:
        if is_duplicate_news(article, history):
            duplicate_count += 1
        else:
            filtered.append(article)
    
    if duplicate_count > 0:
        print(f"âš ï¸ å‘ç° {duplicate_count} æ¡é‡å¤æ–°é—»ï¼ˆå·²è¿‡æ»¤ï¼‰")
    else:
        print("âœ… æœªå‘ç°é‡å¤æ–°é—»")
    print(f"âœ… ä¿ç•™ {len(filtered)} æ¡æ–°é—»ç”¨äºæ‘˜è¦ç”Ÿæˆ")
    return filtered, duplicate_count


def update_news_history(articles, history):
    """Add new articles to news history"""
    today = datetime.now().strftime("%Y-%m-%d")
    for article in articles:
        url = article.get('url', '')
        title = article.get('title', '')
        if not url or not title:
            continue
        existing = False
        for news in history.get("news_history", []):
            if news.get("url") == url:
                news["last_seen"] = today
                existing = True
                break
        if not existing:
            history["news_history"].append({
                "url": url,
                "title": title,
                "title_hash": get_title_hash(title),
                "first_seen": today,
                "last_seen": today
            })
    return history


def fetch_ai_news(query="artificial intelligence AI news", count=20, freshness="pd"):
    """Fetch AI news from Brave Search API"""
    headers = {
        "X-Subscription-Token": BRAVE_API_KEY,
        "Accept": "application/json"
    }
    params = {
        "q": query,
        "count": count,
        "search_lang": "en",
        "freshness": freshness
    }
    
    try:
        print(f"æ­£åœ¨ä» Brave Search API è·å– AI æ–°é—»...")
        print(f"ğŸ“Š API é…é¢: å…è´¹ç‰ˆ - æ¯æœˆ 2000 æ¬¡è¯·æ±‚")
        response = requests.get(BRAVE_BASE_URL, headers=headers, params=params, timeout=30)
        
        if 'x-ratelimit-remaining' in response.headers:
            remaining = response.headers['x-ratelimit-remaining']
            print(f"ğŸ“Š å‰©ä½™é…é¢: {remaining}")
        
        if response.status_code == 200:
            data = response.json()
            articles = []
            
            if 'web' in data and 'results' in data['web']:
                for result in data['web']['results']:
                    articles.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'description': result.get('description', ''),
                        'age': result.get('age', ''),
                        'source': 'web'
                    })
            
            if 'news' in data and 'results' in data['news']:
                for result in data['news']['results']:
                    articles.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'description': result.get('description', ''),
                        'age': result.get('age', ''),
                        'source': 'news'
                    })
            
            print(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles
        else:
            print(f"âŒ API é”™è¯¯: çŠ¶æ€ç  {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ è·å–æ–°é—»æ—¶å‡ºé”™: {type(e).__name__}: {str(e)}")
        return []


def categorize_and_summarize_news(articles):
    """Use LLM to categorize and summarize AI news articles"""
    if not articles:
        return None
    
    articles_text = ""
    for i, article in enumerate(articles[:20], 1):
        articles_text += f"[{i}] æ ‡é¢˜: {article['title']}\n"
        articles_text += f"    é“¾æ¥: {article['url']}\n"
        articles_text += f"    æè¿°: {article['description']}\n\n"
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ AI æ–°é—»ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„å‘¨åº¦æ–°é—»æ‘˜è¦ã€‚

## âš ï¸ é‡è¦ï¼šä¸€çº§åˆ†ç±»é¡ºåº
å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é¡ºåºç»„ç»‡å†…å®¹ï¼š
1. ğŸ‡¨ğŸ‡³ ä¸­å›½
2. ğŸ‡ºğŸ‡¸ ç¾å›½
3. ğŸŒ å…¶ä»–

## äºŒçº§åˆ†ç±»
åœ¨æ¯ä¸ªä¸€çº§åˆ†ç±»ä¸‹ï¼Œå†æŒ‰ä»¥ä¸‹ä¸»é¢˜åˆ†ç±»ï¼š
- ğŸš€ **é‡å¤§çªç ´ä¸äº§å“å‘å¸ƒ**
- ğŸ’¼ **å•†ä¸šåŠ¨æ€ä¸æŠ•èµ„**
- ğŸ”¬ **ç ”ç©¶è¿›å±•**
- ğŸ“Š **è¡Œä¸šè¶‹åŠ¿ä¸åˆ†æ**
- âš–ï¸ **æ”¿ç­–æ³•è§„**
- ğŸŒ **ç¤¾ä¼šå½±å“**

## æ–°é—»æ ¼å¼è¦æ±‚
æ¯æ¡æ–°é—»å¿…é¡»åŒ…å«ï¼š
1. åºå·
2. æ ‡é¢˜ï¼ˆä¸­æ–‡æ¦‚æ‹¬ï¼‰
3. æ ¸å¿ƒè¦ç‚¹ï¼ˆ1-2 å¥è¯ï¼‰
4. **æ¥æº**: [åŸå§‹æ ‡é¢˜](åŸå§‹URL) - å¿…é¡»å¸¦å®Œæ•´é“¾æ¥

## å®Œæ•´æ ¼å¼ç¤ºä¾‹ï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤é¡ºåºï¼‰
### ğŸ‡¨ğŸ‡³ ä¸­å›½

#### ğŸš€ é‡å¤§çªç ´ä¸äº§å“å‘å¸ƒ
**1. å­—èŠ‚è·³åŠ¨å‘å¸ƒæ–°ç‰ˆ AI åŠ©æ‰‹**
- æ–°ç‰ˆåŠ©æ‰‹åœ¨ä¸­æ–‡ç†è§£å’Œå¤šè½®å¯¹è¯æ–¹é¢æœ‰æ˜¾è‘—æå‡
- **æ¥æº**: [å­—èŠ‚è·³åŠ¨å®˜æ–¹å…¬å‘Š](https://www.bytedance.com/news/xxx)

#### ğŸ’¼ å•†ä¸šåŠ¨æ€ä¸æŠ•èµ„
**2. é˜¿é‡Œäº‘å®Œæˆæ–°ä¸€è½®èèµ„**
- ä¼°å€¼çªç ´ 500 äº¿äººæ°‘å¸ï¼Œèµ„é‡‘å°†ç”¨äº AI åŸºç¡€è®¾æ–½å»ºè®¾
- **æ¥æº**: [36æ°ªæŠ¥é“](https://36kr.com/news/xxx)

### ğŸ‡ºğŸ‡¸ ç¾å›½

#### ğŸš€ é‡å¤§çªç ´ä¸äº§å“å‘å¸ƒ
**1. OpenAI å‘å¸ƒ GPT-5 é¢„è§ˆç‰ˆ**
- æ–°æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢æœ‰çªç ´æ€§è¿›å±•
- **æ¥æº**: [OpenAI Blog](https://openai.com/blog/gpt-5)

### ğŸŒ å…¶ä»–

#### ğŸ’¼ å•†ä¸šåŠ¨æ€ä¸æŠ•èµ„
**1. æ¬§æ´² AI å…¬å¸å®Œæˆèèµ„**
- ä¼°å€¼è¾¾åˆ° 10 äº¿æ¬§å…ƒ
- **æ¥æº**: [Reuters](https://reuters.com/xxx)

---

## æ–°é—»åˆ—è¡¨
{articles_text}

è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆæ–°é—»æ‘˜è¦ï¼ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œæ‰€æœ‰é“¾æ¥å¿…é¡»å®Œæ•´å¯ç‚¹å‡»ï¼‰ï¼š
- æ¯ä¸ªä¸€çº§åˆ†ç±»æ ‡é¢˜æ¸…æ™°
- æ¯ä¸ªäºŒçº§åˆ†ç±»ç”¨ emoji æ ‡è¯†
- æ‰€æœ‰é“¾æ¥å¿…é¡»å®Œæ•´å¯ç‚¹å‡»
- ä¼˜å…ˆé€‰æ‹©æœ€é‡è¦çš„æ–°é—»ï¼ˆæ¯ä¸ªäºŒçº§åˆ†ç±» 2-4 æ¡ï¼‰
- å¦‚æœæŸä¸ªåˆ†ç±»æ— æ–°é—»ï¼Œæ ‡æ³¨"æœ¬å‘¨æš‚æ— é‡å¤§åŠ¨æ€"
"""
    
    try:
        print("æ­£åœ¨ä½¿ç”¨ AI ç”Ÿæˆæ–°é—»æ‘˜è¦...")
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=3000
        )
        summary = response.choices[0].message.content
        print("âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ")
        return summary
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return None


def generate_markdown_report(summary, articles, output_file=None, duplicate_count=0):
    """Generate a formatted Markdown report"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    markdown = f"""# AI æ¯å‘¨æ–°é—»æ‘˜è¦
**æ—¥æœŸ**: {today}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**æ–°é—»æ¥æº**: Brave Search API
**æ–‡ç« æ•°é‡**: {len(articles)} ç¯‡
**è¿‡æ»¤é‡å¤**: {duplicate_count} ç¯‡

---

## ğŸ“° ä»Šæ—¥æ‘˜è¦

{summary}

---

## ğŸ“š å®Œæ•´æ–°é—»åˆ—è¡¨

ä»¥ä¸‹æ˜¯æœ¬å‘¨æ”¶é›†çš„æ‰€æœ‰ AI ç›¸å…³æ–°é—»æ–‡ç« ï¼š

"""
    
    for i, article in enumerate(articles, 1):
        markdown += f"### {i}. {article['title']}\n\n"
        markdown += f"**é“¾æ¥**: [{article['url']}]({article['url']})\n\n"
        if article.get('age'):
            markdown += f"**å‘å¸ƒæ—¶é—´**: {article['age']}\n\n"
        markdown += f"**ç®€ä»‹**: {article['description']}\n\n"
        markdown += "---\n\n"
    
    markdown += f"""
*æœ¬æŠ¥å‘Šç”± AI æ¯å‘¨æ–°é—»æ‘˜è¦ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*API ä½¿ç”¨: Brave Search API å…è´¹ç‰ˆ (æ¯æœˆ 2000 æ¬¡è¯·æ±‚é™åˆ¶)*
"""
    
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
    
    return markdown


def main():
    """Main function to generate weekly AI news summary"""
    print("=" * 80)
    print("AI æ¯å‘¨æ–°é—»æ‘˜è¦ç”Ÿæˆå™¨ï¼ˆå¸¦å»é‡åŠŸèƒ½ï¼‰")
    print("=" * 80)
    print()
    
    # Check if already executed this week
    if check_daily_execution():
        today_str = datetime.now().strftime("%Y%m%d")
        existing_file = f"ai_news_summary_{today_str}.md"
        if os.path.exists(existing_file):
            print(f"ğŸ“„ ä»Šæ—¥æŠ¥å‘Šå·²å­˜åœ¨: {existing_file}")
        sys.exit(0)
    
    # Load and cleanup history
    print("æ­£åœ¨åŠ è½½æ–°é—»å†å²è®°å½•...")
    history = load_news_history()
    history = cleanup_old_history(history, days=7)
    print()
    
    # Fetch news
    articles = fetch_ai_news(
        query="artificial intelligence AI news latest",
        count=20,
        freshness="pw"  # Past week
    )
    if not articles:
        print("âŒ é”™è¯¯: æœªèƒ½è·å–æ–°é—»æ–‡ç« ")
        sys.exit(1)
    print()
    
    # Filter duplicates
    filtered_articles, duplicate_count = filter_duplicate_news(articles, history)
    if not filtered_articles:
        print("âš ï¸ æ‰€æœ‰æ–°é—»éƒ½æ˜¯é‡å¤çš„ï¼Œæœ¬å‘¨æ— æ–°å†…å®¹")
        sys.exit(0)
    print()
    
    # Update history
    print("ğŸ“ æ›´æ–°å†å²è®°å½•...")
    history = update_news_history(filtered_articles, history)
    save_news_history(history)
    print()
    
    # Generate summary
    summary = categorize_and_summarize_news(filtered_articles)
    if not summary:
        print("âŒ é”™è¯¯: æœªèƒ½ç”Ÿæˆæ‘˜è¦")
        sys.exit(1)
    print()
    
    # Generate report
    today_str = datetime.now().strftime("%Y%m%d")
    output_file = f"ai_news_summary_{today_str}.md"
    report = generate_markdown_report(summary, filtered_articles, output_file, duplicate_count)
    
    # Push to Feishu
    if FEISHU_ENABLED:
        print()
        print("æ­£åœ¨æ¨é€åˆ°é£ä¹¦...")
        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        try:
            if send_news_summary(today, len(filtered_articles), summary, output_file):
                print("âœ… é£ä¹¦æ¨é€æˆåŠŸï¼")
            else:
                print("âš ï¸ é£ä¹¦æ¨é€å¤±è´¥ï¼Œä½†æŠ¥å‘Šå·²ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
    
    # Update lock file
    update_lock_file()
    
    print()
    print("=" * 80)
    print("âœ… AI æ¯å‘¨æ–°é—»æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
    print("=" * 80)
    print()
    print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {output_file}")
    print(f"ğŸ“Š æ–‡ç« æ•°é‡: {len(filtered_articles)}")
    print(f"ğŸ” è¿‡æ»¤é‡å¤: {duplicate_count} ç¯‡")
    if FEISHU_ENABLED:
        print("ğŸ“± é£ä¹¦æ¨é€: å·²å¯ç”¨")
    print()
    print("ğŸ’¡ æç¤º: è¯¥è„šæœ¬æ¯å‘¨åªèƒ½è¿è¡Œä¸€æ¬¡ï¼Œä»¥ä¿æŠ¤ API é…é¢")
    print("ğŸ’¡ Brave Search API å…è´¹ç‰ˆé™åˆ¶: æ¯æœˆ 2000 æ¬¡è¯·æ±‚")
    print()


if __name__ == "__main__":
    main()
