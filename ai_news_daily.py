#!/usr/bin/env python3
"""
AI Daily News Summary Generator with Deduplication
Uses Brave Search API to fetch latest AI news and generates a comprehensive summary

IMPORTANT: This script is designed to run ONCE per day to stay within API limits
- Brave Search API Free Tier: 2000 requests/month, 1 request/second
- Daily execution: ~30 requests/month (well within limits)

NEW FEATURE: Deduplication
- Prevents duplicate news within the same day
- Prevents duplicate news within the past week
- Uses URL and title similarity for detection
"""

import requests
import json
import os
import sys
import hashlib
from datetime import datetime, timedelta
from openai import OpenAI

# Import Feishu push module
try:
    from feishu_push import send_news_summary
    FEISHU_ENABLED = True
except ImportError:
    FEISHU_ENABLED = False
    print("è­¦å‘Š: é£ä¹¦æ¨é€æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡é£ä¹¦æ¨é€åŠŸèƒ½")

# Configuration - Read from environment variables for GitHub Actions
BRAVE_API_KEY = os.getenv("BRAVE_API_KEY" )
if not BRAVE_API_KEY:
    print("âŒ é”™è¯¯: BRAVE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    print("ğŸ’¡ è¯·åœ¨ GitHub Secrets ä¸­é…ç½® BRAVE_API_KEY")
    sys.exit(1)

BRAVE_BASE_URL = "https://api.search.brave.com/res/v1/web/search"
LOCK_FILE = os.getenv("LOCK_FILE", ".ai_news_lock")
HISTORY_FILE = os.getenv("HISTORY_FILE", ".ai_news_history.json")

# Initialize OpenAI client
# In GitHub Actions, OPENAI_API_KEY and OPENAI_BASE_URL are set as environment variables
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL")
if api_key and base_url:
    client = OpenAI(api_key=api_key, base_url=base_url)
else:
    client = OpenAI()  # Use default configuration

def check_daily_execution():
    """
    Check if the script has already been executed today
    Returns True if already executed, False otherwise
    """
    if os.path.exists(LOCK_FILE):
        try:
            with open(LOCK_FILE, 'r') as f:
                last_run = f.read().strip()
                last_run_date = datetime.strptime(last_run, "%Y-%m-%d").date()
                today = datetime.now().date()
                
                if last_run_date == today:
                    print(f"âš ï¸  è„šæœ¬ä»Šå¤©å·²ç»è¿è¡Œè¿‡äº†ï¼ˆ{last_run}ï¼‰")
                    print(f"âš ï¸  ä¸ºäº†ä¿æŠ¤ API é…é¢ï¼Œæ¯å¤©åªèƒ½è¿è¡Œä¸€æ¬¡")
                    print(f"âš ï¸  å¦‚éœ€å¼ºåˆ¶è¿è¡Œï¼Œè¯·åˆ é™¤é”æ–‡ä»¶: {LOCK_FILE}")
                    return True
        except Exception as e:
            print(f"è­¦å‘Š: è¯»å–é”æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return False

def update_lock_file():
    """
    Update the lock file with today's date
    """
    try:
        with open(LOCK_FILE, 'w') as f:
            f.write(datetime.now().strftime("%Y-%m-%d"))
    except Exception as e:
        print(f"è­¦å‘Š: æ›´æ–°é”æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def load_news_history():
    """
    Load news history from JSON file
    Returns a dictionary with news history
    """
    if not os.path.exists(HISTORY_FILE):
        print("ğŸ“ å†å²è®°å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        return {"news_history": [], "last_cleanup": datetime.now().strftime("%Y-%m-%d")}
    
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
            print(f"âœ… åŠ è½½äº† {len(history.get('news_history', []))} æ¡å†å²æ–°é—»è®°å½•")
            return history
    except Exception as e:
        print(f"âš ï¸  è¯»å–å†å²è®°å½•æ—¶å‡ºé”™: {e}")
        print("ğŸ“ å°†åˆ›å»ºæ–°çš„å†å²è®°å½•æ–‡ä»¶")
        return {"news_history": [], "last_cleanup": datetime.now().strftime("%Y-%m-%d")}

def save_news_history(history):
    """
    Save news history to JSON file
    """
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"âœ… å†å²è®°å½•å·²æ›´æ–°ï¼ˆå…± {len(history.get('news_history', []))} æ¡ï¼‰")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜å†å²è®°å½•æ—¶å‡ºé”™: {e}")

def cleanup_old_history(history, days=7):
    """
    Remove news older than specified days from history
    """
    cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    original_count = len(history.get("news_history", []))
    
    history["news_history"] = [
        news for news in history.get("news_history", [])
        if news.get("last_seen", "2000-01-01") >= cutoff_date
    ]
    
    removed_count = original_count - len(history["news_history"])
    if removed_count > 0:
        print(f"ğŸ§¹ æ¸…ç†äº† {removed_count} æ¡è¶…è¿‡ {days} å¤©çš„æ—§è®°å½•")
    
    history["last_cleanup"] = datetime.now().strftime("%Y-%m-%d")
    return history

def calculate_title_similarity(title1, title2):
    """
    Calculate similarity between two titles using Jaccard similarity
    Returns a value between 0 and 1
    """
    # Convert to lowercase and split into words
    words1 = set(title1.lower().split())
    words2 = set(title2.lower().split())
    
    # Calculate Jaccard similarity
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    if not union:
        return 0.0
    
    return len(intersection) / len(union)

def get_title_hash(title):
    """
    Generate a hash for the title for quick comparison
    """
    return hashlib.md5(title.lower().encode('utf-8')).hexdigest()

def is_duplicate_news(article, history, similarity_threshold=0.85):
    """
    Check if an article is a duplicate based on URL or title similarity
    
    Args:
        article: Article dictionary with 'url' and 'title'
        history: News history dictionary
        similarity_threshold: Threshold for title similarity (0-1)
    
    Returns:
        True if duplicate, False otherwise
    """
    url = article.get('url', '')
    title = article.get('title', '')
    
    if not url or not title:
        return False
    
    # Check URL exact match
    for news in history.get("news_history", []):
        if news.get("url") == url:
            return True
    
    # Check title similarity
    for news in history.get("news_history", []):
        similarity = calculate_title_similarity(title, news.get("title", ""))
        if similarity >= similarity_threshold:
            return True
    
    return False

def filter_duplicate_news(articles, history):
    """
    Filter out duplicate news from articles list
    
    Args:
        articles: List of article dictionaries
        history: News history dictionary
    
    Returns:
        Tuple of (filtered_articles, duplicate_count)
    """
    print("ğŸ” æ­£åœ¨æ£€æŸ¥é‡å¤æ–°é—»...")
    
    filtered = []
    duplicate_count = 0
    
    for article in articles:
        if is_duplicate_news(article, history):
            duplicate_count += 1
        else:
            filtered.append(article)
    
    if duplicate_count > 0:
        print(f"âš ï¸  å‘ç° {duplicate_count} æ¡é‡å¤æ–°é—»ï¼ˆå·²è¿‡æ»¤ï¼‰")
    else:
        print("âœ… æœªå‘ç°é‡å¤æ–°é—»")
    
    print(f"âœ… ä¿ç•™ {len(filtered)} æ¡æ–°é—»ç”¨äºæ‘˜è¦ç”Ÿæˆ")
    
    return filtered, duplicate_count

def update_news_history(articles, history):
    """
    Add new articles to news history
    """
    today = datetime.now().strftime("%Y-%m-%d")
    
    for article in articles:
        url = article.get('url', '')
        title = article.get('title', '')
        
        if not url or not title:
            continue
        
        # Check if already exists
        existing = False
        for news in history.get("news_history", []):
            if news.get("url") == url:
                news["last_seen"] = today
                existing = True
                break
        
        # Add new entry
        if not existing:
            history["news_history"].append({
                "url": url,
                "title": title,
                "title_hash": get_title_hash(title),
                "first_seen": today,
                "last_seen": today
            })
    
    return history

def fetch_ai_news(query="artificial intelligence AI news", count=20, freshness="pd"):
    """
    Fetch AI news from Brave Search API
    
    Args:
        query: Search query string
        count: Number of results to fetch
        freshness: Time filter (pd=past day, pw=past week, pm=past month)
    
    Returns:
        List of news articles with title, url, description, and age
    """
    headers = {
        "X-Subscription-Token": BRAVE_API_KEY,
        "Accept": "application/json"
    }
    
    params = {
        "q": query,
        "count": count,
        "search_lang": "en",
        "freshness": freshness
    }
    
    try:
        print(f"æ­£åœ¨ä» Brave Search API è·å– AI æ–°é—»...")
        print(f"ğŸ“Š API é…é¢: å…è´¹ç‰ˆ - æ¯æœˆ 2000 æ¬¡è¯·æ±‚")
        response = requests.get(BRAVE_BASE_URL, headers=headers, params=params, timeout=30)
        
        # Check rate limit headers
        if 'x-ratelimit-remaining' in response.headers:
            remaining = response.headers['x-ratelimit-remaining']
            print(f"ğŸ“Š å‰©ä½™é…é¢: {remaining}")
        
        if response.status_code == 200:
            data = response.json()
            articles = []
            
            # Extract web results
            if 'web' in data and 'results' in data['web']:
                for result in data['web']['results']:
                    articles.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'description': result.get('description', ''),
                        'age': result.get('age', ''),
                        'source': 'web'
                    })
            
            # Extract news results if available
            if 'news' in data and 'results' in data['news']:
                for result in data['news']['results']:
                    articles.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'description': result.get('description', ''),
                        'age': result.get('age', ''),
                        'source': 'news'
                    })
            
            print(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles
        else:
            print(f"âŒ API é”™è¯¯: çŠ¶æ€ç  {response.status_code}")
            print(f"å“åº”: {response.text}")
            return []
            
    except Exception as e:
        print(f"âŒ è·å–æ–°é—»æ—¶å‡ºé”™: {type(e).__name__}: {str(e)}")
        return []

def categorize_and_summarize_news(articles):
    """ Use LLM to categorize and summarize AI news articles """
    if not articles:
        return None

    articles_text = ""
    for i, article in enumerate(articles[:20], 1):
        articles_text += f"[{i}] æ ‡é¢˜: {article['title']}\n"
        articles_text += f"    é“¾æ¥: {article['url']}\n"
        articles_text += f"    æè¿°: {article['description']}\n\n"

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ AI æ–°é—»ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„å‘¨åº¦æ–°é—»æ‘˜è¦ã€‚

## ä¸€çº§åˆ†ç±»
å°†æ‰€æœ‰æ–°é—»åˆ†ä¸ºä¸‰ä¸ªåœ°ç†åŒºåŸŸï¼š
- ğŸ‡¨ğŸ‡³ **ä¸­å›½** - ä¸­å›½å…¬å¸ã€æœºæ„ã€æ”¿åºœçš„æ–°é—»
- ğŸ‡ºğŸ‡¸ **ç¾å›½** - ç¾å›½å…¬å¸ã€æœºæ„ã€æ”¿åºœï¼ˆä¸å«ä¸­å›½ï¼‰çš„æ–°é—»
- ğŸŒ **å…¶ä»–** - å…¶ä»–å›½å®¶å’Œåœ°åŒºçš„æ–°é—»

## äºŒçº§åˆ†ç±»
åœ¨æ¯ä¸ªä¸€çº§åˆ†ç±»ä¸‹ï¼Œå†æŒ‰ä»¥ä¸‹ä¸»é¢˜åˆ†ç±»ï¼š
- ğŸš€ **é‡å¤§çªç ´ä¸äº§å“å‘å¸ƒ**
- ğŸ’¼ **å•†ä¸šåŠ¨æ€ä¸æŠ•èµ„**
- ğŸ”¬ **ç ”ç©¶è¿›å±•**
- ğŸ“Š **è¡Œä¸šè¶‹åŠ¿ä¸åˆ†æ**
- âš–ï¸ **æ”¿ç­–æ³•è§„**
- ğŸŒ **ç¤¾ä¼šå½±å“**

## æ–°é—»æ ¼å¼è¦æ±‚
æ¯æ¡æ–°é—»å¿…é¡»åŒ…å«ï¼š
1. åºå·
2. æ ‡é¢˜ï¼ˆä¸­æ–‡æ¦‚æ‹¬ï¼‰
3. æ ¸å¿ƒè¦ç‚¹ï¼ˆ1-2 å¥è¯ï¼‰
4. **æ¥æº**: [åŸå§‹æ ‡é¢˜](åŸå§‹URL) - å¿…é¡»å¸¦å®Œæ•´é“¾æ¥

## å®Œæ•´æ ¼å¼ç¤ºä¾‹
### ğŸ‡¨ğŸ‡³ ä¸­å›½

#### ğŸš€ é‡å¤§çªç ´ä¸äº§å“å‘å¸ƒ
**1. å­—èŠ‚è·³åŠ¨å‘å¸ƒæ–°ç‰ˆ AI åŠ©æ‰‹**
- æ–°ç‰ˆåŠ©æ‰‹åœ¨ä¸­æ–‡ç†è§£å’Œå¤šè½®å¯¹è¯æ–¹é¢æœ‰æ˜¾è‘—æå‡
- **æ¥æº**: [å­—èŠ‚è·³åŠ¨å®˜æ–¹å…¬å‘Š](https://www.bytedance.com/news/xxx)

#### ğŸ’¼ å•†ä¸šåŠ¨æ€ä¸æŠ•èµ„
**2. é˜¿é‡Œäº‘å®Œæˆæ–°ä¸€è½®èèµ„**
- ä¼°å€¼çªç ´ 500 äº¿äººæ°‘å¸ï¼Œèµ„é‡‘å°†ç”¨äº AI åŸºç¡€è®¾æ–½å»ºè®¾
- **æ¥æº**: [36æ°ªæŠ¥é“](https://36kr.com/news/xxx)

### ğŸ‡ºğŸ‡¸ ç¾å›½

#### ğŸš€ é‡å¤§çªç ´ä¸äº§å“å‘å¸ƒ
**1. OpenAI å‘å¸ƒ GPT-5 é¢„è§ˆç‰ˆ**
- æ–°æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢æœ‰çªç ´æ€§è¿›å±•
- **æ¥æº**: [OpenAI Blog](https://openai.com/blog/gpt-5)

---

## æ–°é—»åˆ—è¡¨
{articles_text}

è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼ç”Ÿæˆæ–°é—»æ‘˜è¦ï¼ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œæ‰€æœ‰é“¾æ¥å¿…é¡»å®Œæ•´å¯ç‚¹å‡»ï¼‰ï¼š
"""

    try:
        print("æ­£åœ¨ä½¿ç”¨ AI ç”Ÿæˆæ–°é—»æ‘˜è¦...")
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI è¡Œä¸šåˆ†æå¸ˆã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=3000
        )
        summary = response.choices[0].message.content
        print("âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ")
        return summary
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")
        return None

def generate_markdown_report(summary, articles, output_file=None, duplicate_count=0):
    """
    Generate a formatted Markdown report
    
    Args:
        summary: LLM-generated summary text
        articles: List of article dictionaries
        output_file: Output file path (optional)
        duplicate_count: Number of duplicate news filtered
    
    Returns:
        Markdown formatted report string
    """
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    markdown = f"""# AI æ¯æ—¥æ–°é—»æ‘˜è¦

**æ—¥æœŸ**: {today}  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**æ–°é—»æ¥æº**: Brave Search API  
**æ–‡ç« æ•°é‡**: {len(articles)} ç¯‡  
**è¿‡æ»¤é‡å¤**: {duplicate_count} ç¯‡

---

## ğŸ“° ä»Šæ—¥æ‘˜è¦

{summary}

---

## ğŸ“š å®Œæ•´æ–°é—»åˆ—è¡¨

ä»¥ä¸‹æ˜¯ä»Šæ—¥æ”¶é›†çš„æ‰€æœ‰ AI ç›¸å…³æ–°é—»æ–‡ç« ï¼š

"""
    
    for i, article in enumerate(articles, 1):
        markdown += f"\n### {i}. {article['title']}\n\n"
        markdown += f"**é“¾æ¥**: [{article['url']}]({article['url']})\n\n"
        if article.get('age'):
            markdown += f"**å‘å¸ƒæ—¶é—´**: {article['age']}\n\n"
        markdown += f"**ç®€ä»‹**: {article['description']}\n\n"
        markdown += "---\n"
    
    markdown += f"\n\n*æœ¬æŠ¥å‘Šç”± AI æ¯æ—¥æ–°é—»æ‘˜è¦ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*\n"
    markdown += f"\n*API ä½¿ç”¨: Brave Search API å…è´¹ç‰ˆ (æ¯æœˆ 2000 æ¬¡è¯·æ±‚é™åˆ¶)*\n"
    markdown += f"\n*å»é‡åŠŸèƒ½: è‡ªåŠ¨è¿‡æ»¤ä¸€å‘¨å†…çš„é‡å¤æ–°é—»*\n"
    
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
    
    return markdown

def main():
    """
    Main function to generate daily AI news summary
    """
    print("=" * 80)
    print("AI æ¯æ—¥æ–°é—»æ‘˜è¦ç”Ÿæˆå™¨ï¼ˆå¸¦å»é‡åŠŸèƒ½ï¼‰")
    print("=" * 80)
    print()
    
    # Check if already executed today
    if check_daily_execution():
        today_str = datetime.now().strftime("%Y%m%d")
        existing_file = f"/home/ubuntu/ai_news_summary_{today_str}.md"
        if os.path.exists(existing_file):
            print(f"ğŸ“„ ä»Šæ—¥æŠ¥å‘Šå·²å­˜åœ¨: {existing_file}")
        sys.exit(0)
    
    # Step 1: Load and cleanup news history
    print("æ­£åœ¨åŠ è½½æ–°é—»å†å²è®°å½•...")
    history = load_news_history()
    history = cleanup_old_history(history, days=7)
    print()
    
    # Step 2: Fetch news from Brave Search API
    articles = fetch_ai_news(
        query="artificial intelligence AI news latest",
        count=20,
        freshness="pd"  # Past day
    )
    
    if not articles:
        print("âŒ é”™è¯¯: æœªèƒ½è·å–æ–°é—»æ–‡ç« ")
        sys.exit(1)
    
    print()
    
    # Step 3: Filter duplicate news
    filtered_articles, duplicate_count = filter_duplicate_news(articles, history)
    
    if not filtered_articles:
        print("âš ï¸  æ‰€æœ‰æ–°é—»éƒ½æ˜¯é‡å¤çš„ï¼Œä»Šæ—¥æ— æ–°å†…å®¹")
        print("ğŸ’¡ ä¸ä¼šç”ŸæˆæŠ¥å‘Šå’Œæ¨é€é€šçŸ¥")
        sys.exit(0)
    
    print()
    
    # Step 4: Update news history
    print("ğŸ“ æ›´æ–°å†å²è®°å½•...")
    history = update_news_history(filtered_articles, history)
    save_news_history(history)
    print()
    
    # Step 5: Generate summary using LLM
    summary = categorize_and_summarize_news(filtered_articles)
    
    if not summary:
        print("âŒ é”™è¯¯: æœªèƒ½ç”Ÿæˆæ‘˜è¦")
        sys.exit(1)
    
    print()
    
    # Step 6: Generate Markdown report
    today_str = datetime.now().strftime("%Y%m%d")
    output_file = f"ai_news_summary_{today_str}.md"
    
    report = generate_markdown_report(summary, filtered_articles, output_file, duplicate_count)
    
    # Step 7: Push to Feishu
    if FEISHU_ENABLED:
        print()
        print("æ­£åœ¨æ¨é€åˆ°é£ä¹¦...")
        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        try:
            if send_news_summary(today, len(filtered_articles), summary, output_file):
                print("âœ… é£ä¹¦æ¨é€æˆåŠŸï¼")
            else:
                print("âš ï¸  é£ä¹¦æ¨é€å¤±è´¥ï¼Œä½†æŠ¥å‘Šå·²ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸  é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
    
    # Step 8: Update lock file to prevent multiple executions
    update_lock_file()
    
    print()
    print("=" * 80)
    print("âœ… AI æ¯æ—¥æ–°é—»æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
    print("=" * 80)
    print()
    print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {output_file}")
    print(f"ğŸ“Š æ–‡ç« æ•°é‡: {len(filtered_articles)}")
    print(f"ğŸ” è¿‡æ»¤é‡å¤: {duplicate_count} ç¯‡")
    if FEISHU_ENABLED:
        print("ğŸ“± é£ä¹¦æ¨é€: å·²å¯ç”¨")
    print(f"ğŸ”’ å·²æ›´æ–°é”æ–‡ä»¶ï¼Œä»Šå¤©ä¸ä¼šå†æ¬¡æ‰§è¡Œ")
    print()
    print("ğŸ’¡ æç¤º: è¯¥è„šæœ¬æ¯å¤©åªèƒ½è¿è¡Œä¸€æ¬¡ï¼Œä»¥ä¿æŠ¤ API é…é¢")
    print("ğŸ’¡ Brave Search API å…è´¹ç‰ˆé™åˆ¶: æ¯æœˆ 2000 æ¬¡è¯·æ±‚")
    print("ğŸ’¡ æ¯æ—¥è¿è¡Œä¸€æ¬¡: æ¯æœˆçº¦ 30 æ¬¡è¯·æ±‚ï¼ˆè¿œä½äºé™åˆ¶ï¼‰")
    print("ğŸ’¡ å»é‡åŠŸèƒ½: è‡ªåŠ¨è¿‡æ»¤ä¸€å‘¨å†…çš„é‡å¤æ–°é—»")
    print()

if __name__ == "__main__":
    main()
